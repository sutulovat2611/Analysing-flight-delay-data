{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we will implement Spark Structured Streaming to consume the data from task 1 and perform streaming classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SparkSession is created using a SparkConf object, which would use two local cores with a proper application name, and use UTC as the timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import zipfile as zf\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "spark_conf = SparkConf().setMaster(\"local[2]\").setAppName(\"Task3\").set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. From the Kafka producers in Task 1, ingest the streaming data into Spark Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'flightTopic'\n",
    "flightsDf = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", topic).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Then the streaming data format should be transformed into the proper formats following the file schema in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: string (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "schema = ArrayType(StructType([    \n",
    "    StructField('YEAR', StringType(), True), \n",
    "    StructField('MONTH', StringType(), True), \n",
    "    StructField('DAY', StringType(), True), \n",
    "    StructField('DAY_OF_WEEK', StringType(), True), \n",
    "    StructField('AIRLINE', StringType(), True), \n",
    "    StructField('FLIGHT_NUMBER', StringType(), True), \n",
    "    StructField('TAIL_NUMBER', StringType(), True), \n",
    "    StructField('ORIGIN_AIRPORT', StringType(), True), \n",
    "    StructField('DESTINATION_AIRPORT', StringType(), True),\n",
    "    StructField('SCHEDULED_DEPARTURE', StringType(), True),\n",
    "    StructField('DEPARTURE_TIME', StringType(), True),\n",
    "    StructField('DEPARTURE_DELAY', StringType(), True),\n",
    "    StructField('TAXI_OUT', StringType(), True),\n",
    "    StructField('WHEELS_OFF', StringType(), True),\n",
    "    StructField('SCHEDULED_TIME', StringType(), True),\n",
    "    StructField('ELAPSED_TIME', StringType(), True),\n",
    "    StructField('AIR_TIME', StringType(), True),\n",
    "    StructField('DISTANCE', StringType(), True),\n",
    "    StructField('WHEELS_ON', StringType(), True),\n",
    "    StructField('TAXI_IN', StringType(), True),\n",
    "    StructField('SCHEDULED_ARRIVAL', StringType(), True),\n",
    "    StructField('ARRIVAL_TIME', StringType(), True),\n",
    "    StructField('ARRIVAL_DELAY', StringType(), True),\n",
    "    StructField('DIVERTED', StringType(), True),\n",
    "    StructField('CANCELLED', StringType(), True),\n",
    "    StructField('CANCELLATION_REASON', StringType(), True),\n",
    "    StructField('AIR_SYSTEM_DELAY', StringType(), True),\n",
    "    StructField('SECURITY_DELAY', StringType(), True),\n",
    "    StructField('AIRLINE_DELAY', StringType(), True),\n",
    "    StructField('LATE_AIRCRAFT_DELAY', StringType(), True),\n",
    "    StructField('WEATHER_DELAY', StringType(), True),\n",
    "    StructField('ts', TimestampType(), True)            \n",
    "]))\n",
    "\n",
    "\n",
    "flightsDf = flightsDf.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))\n",
    "flightsDf = flightsDf.select(F.explode(F.col(\"parsed_value\")).alias('parsed_value'))  \n",
    "\n",
    "flightsDf = flightsDf.select(\n",
    "                    F.col(\"parsed_value.YEAR\").cast(\"int\").alias(\"YEAR\"),\n",
    "                    F.col(\"parsed_value.MONTH\").cast(\"int\").alias(\"MONTH\"),\n",
    "                    F.col(\"parsed_value.DAY\").cast(\"int\").alias(\"DAY\"),\n",
    "                    F.col(\"parsed_value.DAY_OF_WEEK\").cast(\"int\").alias(\"DAY_OF_WEEK\"),\n",
    "                    F.col(\"parsed_value.AIRLINE\").alias(\"AIRLINE\"),\n",
    "                    F.col(\"parsed_value.FLIGHT_NUMBER\").alias(\"FLIGHT_NUMBER\"),\n",
    "                    F.col(\"parsed_value.TAIL_NUMBER\").alias(\"TAIL_NUMBER\"),\n",
    "                    F.col(\"parsed_value.ORIGIN_AIRPORT\").alias(\"ORIGIN_AIRPORT\"),\n",
    "                    F.col(\"parsed_value.DESTINATION_AIRPORT\").alias(\"DESTINATION_AIRPORT\"),\n",
    "                    F.col(\"parsed_value.SCHEDULED_DEPARTURE\").cast(\"int\").alias(\"SCHEDULED_DEPARTURE\"),\n",
    "                    F.col(\"parsed_value.DEPARTURE_TIME\").cast(\"int\").alias(\"DEPARTURE_TIME\"),\n",
    "                    F.col(\"parsed_value.DEPARTURE_DELAY\").cast(\"int\").alias(\"DEPARTURE_DELAY\"),\n",
    "                    F.col(\"parsed_value.TAXI_OUT\").cast(\"int\").alias(\"TAXI_OUT\"),\n",
    "                    F.col(\"parsed_value.WHEELS_OFF\").cast(\"int\").alias(\"WHEELS_OFF\"),\n",
    "                    F.col(\"parsed_value.SCHEDULED_TIME\").cast(\"int\").alias(\"SCHEDULED_TIME\"),\n",
    "                    F.col(\"parsed_value.AIR_TIME\").cast(\"int\").alias(\"AIR_TIME\"),\n",
    "                    F.col(\"parsed_value.ELAPSED_TIME\").cast(\"int\").alias(\"ELAPSED_TIME\"),\n",
    "                    F.col(\"parsed_value.DISTANCE\").cast(\"int\").alias(\"DISTANCE\"),\n",
    "                    F.col(\"parsed_value.WHEELS_ON\").cast(\"int\").alias(\"WHEELS_ON\"),\n",
    "                    F.col(\"parsed_value.TAXI_IN\").cast(\"int\").alias(\"TAXI_IN\"),\n",
    "                    F.col(\"parsed_value.SCHEDULED_ARRIVAL\").cast(\"int\").alias(\"SCHEDULED_ARRIVAL\"),\n",
    "                    F.col(\"parsed_value.ARRIVAL_TIME\").cast(\"int\").alias(\"ARRIVAL_TIME\"),\n",
    "                    F.col(\"parsed_value.ARRIVAL_DELAY\").cast(\"int\").alias(\"ARRIVAL_DELAY\"),\n",
    "                    F.col(\"parsed_value.DIVERTED\").cast(\"int\").alias(\"DIVERTED\"),\n",
    "                    F.col(\"parsed_value.CANCELLED\").cast(\"int\").alias(\"CANCELLED\"),\n",
    "                    F.col(\"parsed_value.CANCELLATION_REASON\").alias(\"CANCELLATION_REASON\"),\n",
    "                    F.col(\"parsed_value.AIR_SYSTEM_DELAY\").cast(\"int\").alias(\"AIR_SYSTEM_DELAY\"),\n",
    "                    F.col(\"parsed_value.SECURITY_DELAY\").cast(\"int\").alias(\"SECURITY_DELAY\"),\n",
    "                    F.col(\"parsed_value.AIRLINE_DELAY\").cast(\"int\").alias(\"AIRLINE_DELAY\"),\n",
    "                    F.col(\"parsed_value.LATE_AIRCRAFT_DELAY\").cast(\"int\").alias(\"LATE_AIRCRAFT_DELAY\"),\n",
    "                    F.col(\"parsed_value.WEATHER_DELAY\").cast(\"int\").alias(\"WEATHER_DELAY\"),\n",
    "                    F.col(\"parsed_value.ts\").alias(\"ts\")\n",
    "                )\n",
    "flightsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the output sink for the flight data stream by viewing the output in the console:\n",
    "    \n",
    "    a. Use trigger interval of 5 seconds\n",
    "    \n",
    "    b. Take a screenshot of output in the console, and paste as image in the jupyter notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = flightsDf.writeStream.outputMode(\"append\").format(\"console\").trigger(processingTime='5 seconds').start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. As in Assignment 2A, add and drop the following columns:\n",
    "    \n",
    "    a. Add label column \"binaryArrDelay\"\n",
    "    \n",
    "    b. Drop columns \"CANCELLATION_REASON\", \"AIR_SYSTEM_DELAY\", \"SECURITY_DELAY\", \"AIRLINE_DELAY\", \"LATE_AIRCRAFT_DELAY\", \"WEATHER_DELAY\"\n",
    "    \n",
    "    c. Add DEPT_TIME_FLAG column\n",
    "    \n",
    "    d. Drop records with Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "#a. Add label column \"binaryArrDelay\"\n",
    "flightsDf = flightsDf.withColumn(\"binaryArrDelay\", F.when(F.col(\"ARRIVAL_DELAY\") > 0, 1).otherwise(0))\n",
    "\n",
    "# b. Drop columns \"..\"\n",
    "#list down the columns to be removed\n",
    "columnsToDrop = ['CANCELLATION_REASON', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY'] \n",
    "#dropping the columns from the list\n",
    "flightsDf = flightsDf.drop(*columnsToDrop)\n",
    "\n",
    "# c. Add DEPT_TIME_FLAG column\n",
    "flightsDf = flightsDf.withColumn('DEPT_TIME_FLAG', F.when((F.col('SCHEDULED_DEPARTURE') >= 0) & (F.col('SCHEDULED_DEPARTURE') < 500), \"Midnight\")\n",
    "                                 .when((F.col('SCHEDULED_DEPARTURE') >= 500) & (F.col('SCHEDULED_DEPARTURE') < 1100), \"Morning\")\n",
    "                                 .when((F.col('SCHEDULED_DEPARTURE') >= 1100) & (F.col('SCHEDULED_DEPARTURE') < 1600), \"Afternoon\")\n",
    "                                 .when((F.col('SCHEDULED_DEPARTURE') >= 1600) & (F.col('SCHEDULED_DEPARTURE') < 2000), \"Evening\")\n",
    "                                 .when((F.col('SCHEDULED_DEPARTURE') >= 2000) & (F.col('SCHEDULED_DEPARTURE') <= 2359), \"Night\")\n",
    "                                 .otherwise(\"Invalid value\"))\n",
    "# d. Drop records with Null values\n",
    "flightsDf=flightsDf.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load the machine learning model given (“binary_arrival_delay_pipeline_model.zip”), and use the model to classify whether each flight records are delayed. This is based on the assumption that the data has been labelled, as in the \"binaryArrDelay\" column. \n",
    "\n",
    "Hints: You can import zipfile as zf to load the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "# extracting zip file\n",
    "zip = ZipFile('binary_arrival_delay_pipeline_model.zip')\n",
    "zip.extractall()\n",
    "\n",
    "# load the machine learning model\n",
    "model = PipelineModel.load('binary_arrival_delay_pipeline_model')\n",
    "\n",
    "# classify whether each flight records are delayed\n",
    "prediction = model.transform(flightsDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the classification results, monitor the data following the requirements below. For each DAY_OF_WEEK = ‘1’, DAY_OF_WEEK = ‘2’, and DAY_OF_WEEK = ‘3’, keep track of the classification accuracy for every 20-second window.\n",
    "\n",
    "i. Create an output sink to store the prediction results in memory.\n",
    "\n",
    "ii. Show the prediction results for every 20-second window (you can show partial results). Your results should include the timestamps of every 20-second window, DAY_OF_WEEK (1,2 and 3), prediction accuracies and count/number of flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_cond = lambda cond: F.sum(F.when(cond, 1).otherwise(0)) # converting the conditions to 0 and 1 to count the True values\n",
    "\n",
    "#grouping by the timestamp and day of week\n",
    "prediction =  prediction.groupBy(window(col(\"ts\"),\"2 seconds\"), \"DAY_OF_WEEK\")\n",
    "\n",
    "# find accuracy and count\n",
    "prediction = prediction.agg((cnt_cond(col(\"prediction\") == col('binaryArrDelay'))/count(col('prediction'))).alias(\"accuracy\"), count(col(\"DAY_OF_WEEK\")).alias('count')).sort(desc(\"window\"))\n",
    "\n",
    "# Only for day = 1, day = 2 and day =3\n",
    "prediction = prediction.filter(\"DAY_OF_WEEK < 4\")\n",
    "\n",
    "# make a query\n",
    "prediction_query = prediction.writeStream.outputMode(\"complete\").format(\"memory\").queryName(\"prediction\").trigger(processingTime='2 seconds').start()\n",
    "prediction_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out\n",
    "spark.sql(\"select * from prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
